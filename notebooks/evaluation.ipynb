{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tribly AI Assistant - Retrieval Evaluation\n",
    "\n",
    "This notebook evaluates and compares different retrieval methods:\n",
    "- **Semantic Search**: Uses sentence embeddings for meaning-based retrieval\n",
    "- **Keyword Search**: Uses TF-IDF based text matching (baseline)\n",
    "- **Hybrid Search**: Combines both using Reciprocal Rank Fusion\n",
    "\n",
    "## Metrics Evaluated\n",
    "- **Precision@k**: Fraction of retrieved documents that are relevant\n",
    "- **Recall@k**: Fraction of relevant documents that were retrieved\n",
    "- **MRR**: Mean Reciprocal Rank - position of first relevant result\n",
    "- **Latency**: Response time in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path('.').absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "results_path = Path('../results/metrics/evaluation_results.json')\n",
    "\n",
    "with open(results_path) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded results from: {results_path}\")\n",
    "print(f\"Number of test queries: {results['num_queries']}\")\n",
    "print(f\"Methods evaluated: {list(results['summary'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for method, metrics in results['summary'].items():\n",
    "    summary_data.append({\n",
    "        'Method': method.capitalize(),\n",
    "        'Precision@1': metrics['precision@1'],\n",
    "        'Precision@3': metrics['precision@3'],\n",
    "        'Precision@5': metrics['precision@5'],\n",
    "        'Precision@10': metrics['precision@10'],\n",
    "        'Recall@5': metrics['recall@5'],\n",
    "        'Recall@10': metrics['recall@10'],\n",
    "        'MRR': metrics['mrr'],\n",
    "        'Latency (ms)': metrics['avg_latency_ms']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df.set_index('Method', inplace=True)\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@k Comparison\n",
    "\n",
    "Precision measures how many of the retrieved documents are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Precision@k Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = ['Semantic', 'Keyword', 'Hybrid']\n",
    "k_values = [1, 3, 5, 10]\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.25\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    precisions = [df.loc[method, f'Precision@{k}'] for k in k_values]\n",
    "    ax.bar(x + i*width, precisions, width, label=method)\n",
    "\n",
    "ax.set_xlabel('k (number of retrieved documents)', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision@k Comparison Across Search Methods', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels([f'k={k}' for k in k_values])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/precision_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: results/figures/precision_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall@k Comparison\n",
    "\n",
    "Recall measures what fraction of all relevant documents were retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 2: Recall@k Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "recall_data = {\n",
    "    'k=5': [df.loc[m, 'Recall@5'] for m in methods],\n",
    "    'k=10': [df.loc[m, 'Recall@10'] for m in methods]\n",
    "}\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, recall_data['k=5'], width, label='Recall@5', color='steelblue')\n",
    "ax.bar(x + width/2, recall_data['k=10'], width, label='Recall@10', color='coral')\n",
    "\n",
    "ax.set_xlabel('Search Method', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.set_title('Recall@k Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for i, (r5, r10) in enumerate(zip(recall_data['k=5'], recall_data['k=10'])):\n",
    "    ax.text(i - width/2, r5 + 0.02, f'{r5:.2f}', ha='center', fontsize=9)\n",
    "    ax.text(i + width/2, r10 + 0.02, f'{r10:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/recall_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: results/figures/recall_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reciprocal Rank (MRR) Comparison\n",
    "\n",
    "MRR measures how quickly the first relevant result appears in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 3: MRR Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "mrr_values = [df.loc[m, 'MRR'] for m in methods]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(methods, mrr_values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_xlabel('Search Method', fontsize=12)\n",
    "ax.set_ylabel('Mean Reciprocal Rank', fontsize=12)\n",
    "ax.set_title('MRR Comparison - First Relevant Result Position', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, mrr_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/mrr_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: results/figures/mrr_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Comparison\n",
    "\n",
    "Average response time for each search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 4: Latency Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "latencies = [df.loc[m, 'Latency (ms)'] for m in methods]\n",
    "colors = ['#9b59b6', '#f39c12', '#1abc9c']\n",
    "\n",
    "bars = ax.bar(methods, latencies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_xlabel('Search Method', fontsize=12)\n",
    "ax.set_ylabel('Average Latency (ms)', fontsize=12)\n",
    "ax.set_title('Response Time Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, latencies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{val:.1f}ms', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/latency_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: results/figures/latency_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 5: Performance Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Select key metrics for heatmap\n",
    "heatmap_data = df[['Precision@5', 'Recall@5', 'Recall@10', 'MRR']].T\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu',\n",
    "            linewidths=0.5, ax=ax, cbar_kws={'label': 'Score'})\n",
    "\n",
    "ax.set_title('Retrieval Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Search Method', fontsize=12)\n",
    "ax.set_ylabel('Metric', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/performance_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: results/figures/performance_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Semantic Search** achieves the best precision@1 and MRR, meaning it's best at ranking the most relevant result first.\n",
    "\n",
    "2. **Hybrid Search** provides the best recall@10, successfully finding more relevant documents overall.\n",
    "\n",
    "3. **Keyword Search** is significantly faster (~1ms vs ~66ms for semantic) but has lower quality metrics.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "| Method | Best For | Trade-off |\n",
    "|--------|----------|----------|\n",
    "| Semantic | Quality, relevance | Slower, needs embeddings |\n",
    "| Keyword | Speed, exact matches | Lower semantic understanding |\n",
    "| Hybrid | Balanced performance | Most complex, moderate latency |\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "For the Tribly AI Assistant:\n",
    "- Use **Semantic Search** as the default for best user experience\n",
    "- Use **Hybrid Search** when comprehensive coverage is needed\n",
    "- Use **Keyword Search** for real-time autocomplete or suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string())\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
